# âœ‹ Hand Recognition using CNN
CNN-based static hand gesture classification  
ðŸ“ GitHub: [`rehaan1603/SCT_ML_4`](https://github.com/rehaan1603/SCT_ML_4)

---

## ðŸ“Œ Project Overview

- **Objective**: Build a CNN model to classify static hand gestures from grayscale images.
- **Model**: Convolutional Neural Network (CNN)
- **Dataset**: CSV files (`sign_mnist_train.csv`, `sign_mnist_test.csv`) with 28x28 grayscale images and 25 gesture labels.
- **Platform Used**: Google Colab (Jupyter Notebook environment)

---

## ðŸ› ï¸ Technologies Used

- Python  
- TensorFlow / Keras  
- NumPy  
- Pandas  
- Matplotlib  
- scikit-learn  

---

## ðŸ“ Project Structure

```bash
SCT_ML_4/
â”œâ”€â”€ sign_mnist_train.csv          # Training dataset
â”œâ”€â”€ sign_mnist_test.csv           # Testing dataset
â”œâ”€â”€ hand_gesture_model.ipynb      # CNN training notebook
â”œâ”€â”€ images/
â”‚   â”œâ”€â”€ sample_colored.png
â”‚   â”œâ”€â”€ sample_grayscale.png
â”‚   â””â”€â”€ gesture_reference.png
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ðŸ§  Model Architecture

```text
Input Layer: (28 x 28 x 1)
â†“
Conv2D (32 filters, 3x3) â†’ ReLU â†’ MaxPooling2D (2x2)
â†“
Conv2D (64 filters, 3x3) â†’ ReLU â†’ MaxPooling2D (2x2)
â†“
Flatten
â†“
Dense (128) â†’ ReLU
â†“
Dropout (0.2)
â†“
Dense (25) â†’ Softmax
```

- **Loss Function**: Sparse Categorical Crossentropy  
- **Optimizer**: Adam  
- **Evaluation Metric**: Accuracy

---

## ðŸ”  Label Mapping

| Label | Gesture |
|-------|---------|
| 0     | A       |
| 1     | B       |
| 2     | C       |
| 3     | D       |
| 4     | E       |
| 5     | F       |
| 6     | G       |
| 7     | H       |
| 8     | I       |
| 9     | K       |
| 10    | L       |
| 11    | M       |
| 12    | N       |
| 13    | O       |
| 14    | P       |
| 15    | Q       |
| 16    | R       |
| 17    | S       |
| 18    | T       |
| 19    | U       |
| 20    | V       |
| 21    | W       |
| 22    | X       |
| 23    | Y       |
| 24    | Z       |

âš ï¸ Gesture `J` is excluded because it requires motion.

---

## ðŸ“Š Results

- âœ… **Training Accuracy**: ~99%  
- âœ… **Test Accuracy**: ~95%  
- ðŸ“ˆ Accuracy and loss graphs plotted over epochs  
- ðŸ–¼ï¸ Sample test predictions visualized using `matplotlib`

---

## ðŸ–¼ï¸ Sample Visuals

| Colored Input | Grayscale | Gesture Chart |
|---------------|-----------|----------------|
| ![Color](images/sample_colored.png) | ![Gray](images/sample_grayscale.png) | ![Chart](images/gesture_reference.png) |

---

## ðŸ”§ Installation

Use a virtual environment or Google Colab to install:

```bash
pip install -r requirements.txt
```

---

## ðŸ“¦ requirements.txt

```txt
tensorflow
numpy
pandas
matplotlib
scikit-learn
```

---

## ðŸš€ Future Scope

- Real-time detection using OpenCV and webcam  
- Web interface with Streamlit or Flask  
- More robust datasets and multi-label support

---

## ðŸ“„ License

MIT License â€” for academic and educational use.

---

## ðŸ™Œ Acknowledgments

- Dataset Source: Kaggle CSV (Static gesture data)
- Training Done on: Google Colab

---

> ðŸ”— GitHub Repo: [`rehaan1603/SCT_ML_4`](https://github.com/rehaan1603/SCT_ML_4)
